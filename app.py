from flask import Flask, render_template, request, send_file
from googleapiclient.discovery import build
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
import torch
import re
from io import BytesIO
from reportlab.pdfgen import canvas
from reportlab.lib.pagesizes import letter
import google.generativeai as genai
import os

app = Flask(__name__)

# Load Sentiment Model (CardiffNLP)
sentiment_model_name = "cardiffnlp/twitter-roberta-base-sentiment-latest"
sentiment_tokenizer = AutoTokenizer.from_pretrained(sentiment_model_name)
sentiment_model = AutoModelForSequenceClassification.from_pretrained(sentiment_model_name)

# Load BART Summarizer
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

# YouTube API Key
Y_key = "AIzaSyALGIdXC88KZq52Tq0YcintijoEHmPF30o"

# Gemini API Key
genai.configure(api_key="AIzaSyBv1WBCVqLxwJf8OB11ZW43493v3hzDakc")

def get_comments(video_id, max_comments=300):
    youtube = build("youtube", "v3", developerKey=Y_key)

    video_response = youtube.videos().list(
        part="statistics",
        id=video_id
    ).execute()
    total_comments = int(video_response["items"][0]["statistics"].get("commentCount", 0))

    comments = []
    next_page_token = None

    while len(comments) < max_comments:
        request = youtube.commentThreads().list(
            part="snippet",
            videoId=video_id,
            maxResults=100,
            pageToken=next_page_token,
            textFormat="plainText"
        )
        response = request.execute()
        for item in response["items"]:
            comment = item["snippet"]["topLevelComment"]["snippet"]["textDisplay"]
            comments.append(comment)
        next_page_token = response.get("nextPageToken")
        if not next_page_token:
            break

    return total_comments, comments[:max_comments]

def analyze_sentiments(comments):
    sentiment_counts = {"Positive": 0, "Negative": 0, "Neutral": 0}
    labeled_comments = []

    for comment in comments:
        try:
            inputs = sentiment_tokenizer(comment, return_tensors="pt", truncation=True, max_length=512)
            with torch.no_grad():
                outputs = sentiment_model(**inputs)
                scores = torch.nn.functional.softmax(outputs.logits, dim=1)[0]
                label = torch.argmax(scores).item()
                polarity = scores[label].item()

            if label == 0:
                sentiment = "Negative"
                polarity = -polarity
            elif label == 2:
                sentiment = "Positive"
            else:
                sentiment = "Neutral"
                polarity = 0

            sentiment_counts[sentiment] += 1
            labeled_comments.append((comment, sentiment, polarity))
        except Exception as e:
            print(f"Skipped a comment due to error: {e}")
            continue

    return sentiment_counts, labeled_comments

def generate_summary_from_distribution(sentiment_counts):
    pos = sentiment_counts.get("Positive", 0)
    neg = sentiment_counts.get("Negative", 0)
    neu = sentiment_counts.get("Neutral", 0)
    total = pos + neg + neu

    if total == 0:
        return "Not enough comments to generate a summary."

    if pos > neg and pos > neu:
        return "👍 The majority of viewers appreciated the video. Public opinion is largely positive."
    elif neg > pos and neg > neu:
        return "👎 The video received more negative reactions than positive ones. Viewers seem dissatisfied."
    elif neu > pos and neu > neg:
        return "😐 Most comments are neutral. The video has moderate engagement or limited impact."
    else:
        return "🤝 Viewer opinions are mixed. The video has both supporters and critics."

def generate_summary_with_gemini_or_bart(comments):
    text = " ".join(comments[:15]).strip()
    if not text:
        return "No sufficient data to summarize."

    try:
        model = genai.GenerativeModel('gemini-pro')
        response = model.generate_content(f"Summarize these YouTube comments:\n{text}")
        return response.text.strip() + " (Generated by Gemini)"
    except Exception as e:
        print(f"Gemini failed: {e}")
        try:
            result = summarizer(text, max_length=100, min_length=30, do_sample=False)
            return result[0]['summary_text'] + " (Fallback summarization used.)"
        except:
            return "Summary generation failed."

@app.route('/')
def home():
    return render_template('home.html')

@app.route('/link')
def link_input():
    return render_template('index.html')

@app.route('/analyze', methods=['POST'])
def analyze():
    Y_link = request.form['link']
    match = re.search(r"(?:v=|\/)([0-9A-Za-z_-]{11})", Y_link)
    if not match:
        return "Invalid YouTube link."

    video_id = match.group(1)
    total_comments, comments = get_comments(video_id)
    sentiment_counts, labeled_comments = analyze_sentiments(comments)

    top_positive = [c[0] for c in sorted(labeled_comments, key=lambda x: x[2], reverse=True) if c[1] == "Positive"][:5]
    top_negative = [c[0] for c in sorted(labeled_comments, key=lambda x: x[2]) if c[1] == "Negative"][:5]

    sentiment_summary = generate_summary_from_distribution(sentiment_counts)
    bart_summary = generate_summary_with_gemini_or_bart([c[0] for c in labeled_comments if c[1] != "Neutral"])

    return render_template(
        'results.html',
        video_id=video_id,
        total_comments=total_comments,
        analyzed_count=len(comments),
        pos=sentiment_counts['Positive'],
        neg=sentiment_counts['Negative'],
        neu=sentiment_counts['Neutral'],
        top_pos=top_positive,
        top_neg=top_negative,
        all_comments=labeled_comments,
        summary_text=sentiment_summary,
        bart_summary=bart_summary,
        original_link=Y_link
    )

@app.route('/download')
def download():
    yt_link = request.args.get('link')
    match = re.search(r"(?:v=|\/)([0-9A-Za-z_-]{11})", yt_link)
    if not match:
        return "Invalid YouTube link."

    video_id = match.group(1)
    _, comments = get_comments(video_id)
    _, analyzed_comments = analyze_sentiments(comments)

    buffer = BytesIO()
    c = canvas.Canvas(buffer, pagesize=letter)
    width, height = letter
    y = height - 40

    c.setFont("Helvetica-Bold", 16)
    c.drawString(30, y, "YouTube Comment Sentiment Report")
    y -= 30
    c.setFont("Helvetica", 12)

    for comment, sentiment, _ in analyzed_comments:
        line = f"[{sentiment}] {comment}"
        while len(line) > 100:
            c.drawString(30, y, line[:100])
            line = line[100:]
            y -= 15
            if y < 40:
                c.showPage()
                y = height - 40
        c.drawString(30, y, line)
        y -= 20

    c.save()
    buffer.seek(0)

    return send_file(
        buffer,
        as_attachment=True,
        download_name="comments_report.pdf",
        mimetype="application/pdf"
    )

if __name__ == '__main__':
    app.run(debug=True)
